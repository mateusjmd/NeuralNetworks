{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7a2a238",
   "metadata": {},
   "source": [
    "Neural Network: Linear Regression\n",
    "===\n",
    "\n",
    "**Autor:** Mateus de Jesus Mendes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d1f306",
   "metadata": {},
   "source": [
    "# Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9edcb23",
   "metadata": {},
   "source": [
    "O presente *Jupyter Notebook* tem por objetivo implementar em Python puro uma Rede Neural Linear de Camada Única Integralmente Conectada para regressão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7606c0",
   "metadata": {},
   "source": [
    "# Metodologia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e4ebc",
   "metadata": {},
   "source": [
    "### Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "6f087845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231665de",
   "metadata": {},
   "source": [
    "### Definições Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "54137896",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 88\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f2cab",
   "metadata": {},
   "source": [
    "### Funções Auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386da65f",
   "metadata": {},
   "source": [
    "Funções secundárias utilizadas nas etapas principais da metodologia. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ab5944",
   "metadata": {},
   "source": [
    "##### **Produto Interno**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "b0c00c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(a, b):\n",
    "    \"\"\"\n",
    "    Cálculo do produto interno dos casos:\n",
    "    - Vetor-vetor\n",
    "    - Matriz-vetor\n",
    "    - Matriz-matriz\n",
    "    \n",
    "    ### Parâmetros\n",
    "    - `a`/`b`: Vetor ou matriz.\n",
    "\n",
    "    ### Retorna\n",
    "    Float (escalar) para produto vetor-vetor / Lista (vetor) para produto matriz-vetor / Matriz (lista de listas) para produto matriz-matriz.\n",
    "    \"\"\"\n",
    "\n",
    "    # Funções auxiliares de identificação\n",
    "    def is_vector(x):\n",
    "        return isinstance(x[0], (int, float))\n",
    "\n",
    "    def is_matrix(x):\n",
    "        return isinstance(x[0], list)\n",
    "\n",
    "    # Produto vetor-vetor -> escalar\n",
    "    if is_vector(a) and is_vector(b):\n",
    "        if len(a) != len(b):\n",
    "            raise ValueError(\"Vetores devem ter o mesmo comprimento.\")\n",
    "        \n",
    "        s = 0\n",
    "        for i in range(len(a)):\n",
    "            s += a[i] * b[i]\n",
    "        return s\n",
    "\n",
    "    # Produto matriz-vetor -> vetor\n",
    "    if is_matrix(a) and is_vector(b):\n",
    "        n_rows = len(a)\n",
    "        n_cols = len(a[0])\n",
    "\n",
    "        if n_cols != len(b):\n",
    "            raise ValueError(\"Dimensões incompatíveis para produto matriz-vetor.\")\n",
    "\n",
    "        result = []\n",
    "        for i in range(n_rows):\n",
    "            sum = 0\n",
    "            for j in range(n_cols):\n",
    "                sum += a[i][j] * b[j]\n",
    "            result.append(sum)\n",
    "        return result\n",
    "\n",
    "    # Produto matriz-matriz -> matriz\n",
    "    if is_matrix(a) and is_matrix(b):\n",
    "        n_rows_a = len(a)\n",
    "        n_cols_a = len(a[0])\n",
    "        n_rows_b = len(b)\n",
    "        n_cols_b = len(b[0])\n",
    "\n",
    "        if n_cols_a != n_rows_b:\n",
    "            raise ValueError(\"Dimensões incompatíveis para produto matriz-matriz.\")\n",
    "\n",
    "        result = []\n",
    "        for i in range(n_rows_a):\n",
    "            row = []\n",
    "            for j in range(n_cols_b):\n",
    "                sum = 0\n",
    "                for k in range(n_cols_a):\n",
    "                    sum += a[i][k] * b[k][j]\n",
    "                row.append(sum)\n",
    "            result.append(row)\n",
    "        return result\n",
    "\n",
    "    # Caso inválido\n",
    "    raise TypeError(\"Entradas devem ser vetores ou matrizes válidas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9949e801",
   "metadata": {},
   "source": [
    "##### ***Holdout***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "57c005cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout(X, y, train_size=0.8):\n",
    "    \"\"\"\n",
    "    Realiza a partição *holdout* de um conjunto de dados supervisionado\n",
    "    em subconjuntos de treinamento e teste.\n",
    "\n",
    "    A função separa aleatoriamente o conjunto de dados original\n",
    "    D = {(x_i, y_i)}_{i=1}^{N} em dois subconjuntos disjuntos:\n",
    "    um conjunto de treinamento D_train e um conjunto de teste D_test,\n",
    "    respeitando a proporção especificada por `train_size`.\n",
    "\n",
    "    ### Parâmetros\n",
    "    - `X` (list[list[float]]): Conjunto de N vetores de entrada, X = {x₁, x₂, …, x_N}, com x_i ∈ ℝᵈ.\n",
    "\n",
    "    - `y` (list[float]): Vetor dos valores alvo associados às instâncias de entrada, y = (y₁, y₂, …, y_N).\n",
    "\n",
    "    - `train_size` (float, opcional): Fração do conjunto de dados destinada ao treinamento, com 0 < `train_size` < 1.\n",
    "\n",
    "    ### Retorno\n",
    "    tuple[list, list, list, list]\n",
    "\n",
    "        Tupla `(X_train, X_test, y_train, y_test)`, onde:\n",
    "     - `X_train`: subconjunto dos vetores de entrada utilizados no treinamento;\n",
    "     - `X_test`: subconjunto dos vetores de entrada utilizados na avaliação;\n",
    "     - `y_train`: valores alvo correspondentes ao conjunto de treinamento;\n",
    "     - `y_test`: valores alvo correspondentes ao conjunto de teste.\n",
    "\n",
    "    ### Observações\n",
    "    - A partição é realizada por embaralhamento aleatório dos índices,\n",
    "      caracterizando um *holdout* simples.\n",
    "    - Não há estratificação em relação à variável alvo.\n",
    "    - Não há controle explícito de *seed*, o que implica não reprodutibilidade\n",
    "      entre execuções distintas.\n",
    "    - A função preserva a correspondência entre instâncias de entrada e seus\n",
    "      respectivos valores alvo.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if not 0 < train_size < 1:\n",
    "        raise ValueError('train_size deve estar no intervalo (0, 1).')\n",
    "    \n",
    "    if len(X) != len(y):\n",
    "        raise ValueError('X e y devem ter o mesmo comprimento.')\n",
    "\n",
    "    n = len(X)\n",
    "    indices = list(range(n))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    split = int(n * train_size)\n",
    "\n",
    "    train_idx = indices[:split]\n",
    "    test_idx = indices[split:]\n",
    "\n",
    "    X_train = [X[i] for i in train_idx]\n",
    "    y_train = [y[i] for i in train_idx]\n",
    "\n",
    "    X_test = [X[i] for i in test_idx]\n",
    "    y_test = [y[i] for i in test_idx]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94bb5b3",
   "metadata": {},
   "source": [
    "### 0. Geração / Leitura de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b361028",
   "metadata": {},
   "source": [
    "O conjunto de dados é gerado a partir do modelo linear com ruído aditivo:\n",
    "$$\n",
    "y_i = \\mathbf{w}_{\\text{true}}^{\\top}\\mathbf{x}_i + b_{\\text{true}} + \\varepsilon_i,\n",
    "\\quad i = 1, \\dots, N\n",
    "$$\n",
    "\n",
    "Em que:\n",
    "- $\\mathbf{x}_i \\in \\mathbb{R}^d$ é o vetor de *features* da $i$-ésima amostra;\n",
    "- $\\mathbf{w}_{\\text{true}} \\in \\mathbb{R}^d$ é o vetor de pesos verdadeiro;\n",
    "- $b_{\\text{true}} \\in \\mathbb{R}$ é o viés verdadeiro;\n",
    "- $\\varepsilon_i \\sim \\mathcal{N}(0, 1)$ representa ruído Gaussiano aditivo independente.\n",
    "\n",
    "Os vetores $\\mathbf{x}_i$ também são amostrados de uma Distribuição Normal Padrão:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "3b830de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synth_data(n_examples, n_features, return_params=False):\n",
    "    \"\"\"\n",
    "    Gera um conjunto de dados sintético para regressão linear com ruído aditivo.\n",
    "\n",
    "    A função constrói um *dataset* artificial a partir do modelo gerador\n",
    "\n",
    "        y_i = w_trueᵀ x_i + b_true + ε_i,\n",
    "\n",
    "    onde x_i ∈ ℝᵈ são vetores de entrada amostrados de uma distribuição Gaussiana,\n",
    "    w_true ∈ ℝᵈ é o vetor de pesos verdadeiro, b_true ∈ ℝ é o viés verdadeiro\n",
    "    e ε_i ∈ ℝ representa ruído aditivo Gaussiano independente.\n",
    "\n",
    "    ### Parâmetros\n",
    "    - `n_examples` (int): Número de instâncias (amostras) a serem geradas, N.\n",
    "\n",
    "    - `n_features` (int): Dimensionalidade do espaço de entrada, d, correspondente ao número de *features* de cada vetor x_i ∈ ℝᵈ.\n",
    "\n",
    "    - `return_params` (bool, opcional): Se verdadeiro, a função retorna também os parâmetros fundamentais utilizados no processo gerador\n",
    "    dos dados (`w_true`, `b_true`).\n",
    "\n",
    "    ### Retorno\n",
    "    tuple\n",
    "\n",
    "    - Se `return_params=False`, retorna a tupla `(X, y)`, onde:\n",
    "        - `X` ∈ ℝ^{N×d} é a coleção de vetores de entrada;\n",
    "        - `y` ∈ ℝ^{N} é o vetor de valores alvo gerados.\n",
    "\n",
    "    - Se `return_params=True`, retorna a tupla `(X, y, w_true, b_true)`, contendo adicionalmente:\n",
    "        - `w_true` ∈ ℝᵈ, vetor de pesos verdadeiro;\n",
    "        - `b_true` ∈ ℝ, viés verdadeiro.\n",
    "\n",
    "    ### Observações\n",
    "    - Os vetores de entrada `x_i` e o ruído ε_i são amostrados de distribuições\n",
    "      Gaussianas independentes com média zero e variância unitária.\n",
    "    - O viés verdadeiro `b_true` é fixado como uma constante escalar.\n",
    "    - A função pressupõe a existência de rotinas externas para amostragem\n",
    "      Gaussiana (`sample_gaussian`) e cálculo de produto interno (`dot`).\n",
    "    - Não há controle explícito de *seed*, o que implica não reprodutibilidade\n",
    "      entre execuções distintas.\n",
    "    \"\"\"\n",
    "\n",
    "    X = [] # Matriz de instâncias e features\n",
    "    y = [] # Vetor de targets\n",
    "\n",
    "    # Vetor verdadeiro de pesos\n",
    "    w_true = [random.gauss() for _ in range(n_features)]\n",
    "\n",
    "    # Viés verdadeiro\n",
    "    b_true = 8.0\n",
    "\n",
    "    for _ in range(n_examples):\n",
    "        x_i = [random.gauss() for _ in range(n_features)]\n",
    "\n",
    "        noise = random.gauss()\n",
    "\n",
    "        y_i = dot(w_true, x_i) + b_true + noise\n",
    "\n",
    "        X.append(x_i)\n",
    "        y.append(y_i)\n",
    "\n",
    "    if return_params:\n",
    "        return X, y, w_true, b_true\n",
    "    else:\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a16b712",
   "metadata": {},
   "source": [
    "### 1. Definição do Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139271d3",
   "metadata": {},
   "source": [
    "O modelo considerado corresponde a uma transformação afim totalmente conectada, definida por:\n",
    "$$\n",
    "\\hat{y} = \\mathbf{w}^{\\top}\\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "Em que:\n",
    "- $\\mathbf{x} \\in \\mathbb{R}^d$ é o vetor de entrada;\n",
    "- $\\mathbf{w} \\in \\mathbb{R}^d$ é o vetor de pesos treináveis;\n",
    "- $b \\in \\mathbb{R}$ é o termo de viés.\n",
    "\n",
    "Os parâmetros $\\mathbf{w}$ e $b$ são inicializados aleatoriamente a partir de uma distribuição uniforme simétrica de pequeno suporte em torno de zero:\n",
    "$$\n",
    "w_j \\sim \\mathcal{U}(-\\epsilon, \\epsilon),\n",
    "\\quad\n",
    "b \\sim \\mathcal{U}(-\\epsilon, \\epsilon),\n",
    "\\quad\n",
    "\\epsilon = 10^{-3}\n",
    "$$\n",
    "\n",
    "Essa abordatem tem por objetivo garantir estabilidade numérica e dinâmica adequada do gradiente nas etapas iniciais do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "9e93c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(n_features):\n",
    "    \"\"\"\n",
    "    Inicializa os parâmetros treináveis de um modelo linear (camada totalmente conectada).\n",
    "\n",
    "    A função constrói os parâmetros de uma transformação afim definida por\n",
    "\n",
    "        ŷ = wᵀ x + b,\n",
    "\n",
    "    onde w ∈ ℝᵈ é o vetor de pesos e b ∈ ℝ é o termo de viés. Os parâmetros são\n",
    "    inicializados com valores aleatórios de pequena magnitude, centrados em zero,\n",
    "    de modo a favorecer estabilidade numérica durante as primeiras etapas do\n",
    "    treinamento baseado em gradiente.\n",
    "\n",
    "    ### Parâmetros\n",
    "    - `n_features` (int): Dimensionalidade do espaço de entrada (d), correspondente ao número de *features* de cada vetor x ∈ ℝᵈ.\n",
    "\n",
    "    ### Retorno\n",
    "    tuple[list[float], float]\n",
    "\n",
    "        Tupla `(w, b)` contendo:\n",
    "\n",
    "    - `w`: vetor de pesos inicializado aleatoriamente, w ∈ ℝᵈ;\n",
    "    - `b`: escalar correspondente ao viés do modelo.\n",
    "\n",
    "    ### Observações\n",
    "    - A inicialização utiliza uma distribuição uniforme simétrica em torno de zero\n",
    "      com suporte reduzido, o que é consistente com práticas básicas de inicialização\n",
    "      para modelos lineares.\n",
    "    - A função não depende da distribuição dos dados nem do tamanho do *batch*.\n",
    "    - Não há controle explícito de *seed* para reprodutibilidade.\n",
    "    \"\"\"\n",
    "\n",
    "    # Inicializa o vetor de pesos com valores centrados em 0\n",
    "    w = [random.uniform(-1e-3, 1e-3) for _ in range(n_features)]\n",
    "\n",
    "    # Inicializa o viés com valor centrado em 0\n",
    "    b = random.uniform(-1e-3, 1e-3)\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f2d2e",
   "metadata": {},
   "source": [
    "### 2. *Forward Pass*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6810d877",
   "metadata": {},
   "source": [
    "Dado um conjunto de $N$ vetores de entrada, o *forward pass* do modelo linear consiste na aplicação de uma transformação afim independente a cada instância:\n",
    "$$\n",
    "\\hat{y}_i = \\mathbf{w}^{\\top}\\mathbf{x}_i + b\n",
    "\\quad i = 1,\\dots,N\n",
    "$$\n",
    "\n",
    "Em forma vetorial:\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = \\mathbf{X}\\mathrm{w} + b\n",
    "$$\n",
    "\n",
    "Em que:\n",
    "- $X \\in \\mathbb{R}^{N \\times d}$ é a matriz de entradas;\n",
    "- $\\mathbf{w} \\in \\mathbb{R}^{d}$ é o vetor de pesos do modelo;\n",
    "- $b \\in \\mathbb{R}$ é o termo de viés;\n",
    "- $\\hat{\\mathbf{y}} \\in \\mathbb{R}^{N}$ é o vetor de saídas pré-ativação (*logits*).\n",
    "\n",
    "Cada saída $\\hat{y}_i$ corresponde à predição linear do modelo para a $i$-ésima amostra, sem aplicação de função de ativação não linear.\n",
    "\n",
    "**Observações conceituais**\n",
    "- Este *forward pass* implementa exatamente uma camada totalmente conectada linear, sendo matematicamente equivalente à regressão linear multivariada.\n",
    "- A ausência de função de ativação implica que o espaço de hipóteses do modelo é estritamente afim.\n",
    "- A independência entre as amostras reflete-se na forma escalar da operação aplicada a cada $\\mathbf{x}_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "2675376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, w, b):\n",
    "    \"\"\"\n",
    "    Executa o *forward pass* de um modelo linear (camada totalmente conectada sem função de ativação).\n",
    "\n",
    "    Dado um conjunto de vetores de entrada, a função aplica uma transformação afim da forma\n",
    "\n",
    "        ŷ_i = wᵀ x_i + b,\n",
    "\n",
    "    onde x_i ∈ ℝᵈ representa a i-ésima instância de entrada, w ∈ ℝᵈ é o vetor de pesos\n",
    "    treináveis do modelo e b ∈ ℝ é o termo de viés. O resultado corresponde às saídas\n",
    "    pré-ativação (*logits*) do modelo.\n",
    "\n",
    "    ### Parâmetros\n",
    "    - `X` (list[list[float]]): Coleção de N vetores de entrada, X = {x₁, x₂, …, x_N}, com x_i ∈ ℝᵈ. Cada sublista representa uma instância \n",
    "    e deve possuir a mesma dimensionalidade do vetor de pesos `w`.\n",
    "\n",
    "    - `w` (list[float]): Vetor de pesos do modelo linear, w ∈ ℝᵈ.\n",
    "\n",
    "    - `b` (float): Escalar correspondente ao termo de viés (*bias*).\n",
    "\n",
    "    ### Retorno\n",
    "    list[float]\n",
    "\n",
    "        Vetor de saídas do modelo, contendo os valores pré-ativação (*logits*)\n",
    "        associados a cada instância de entrada, na mesma ordem em que aparecem em `X`.\n",
    "\n",
    "    ### Observações\n",
    "    - A função implementa uma única camada linear (*fully connected layer*)\n",
    "      sem função de ativação.\n",
    "    - Não há verificação explícita de compatibilidade dimensional entre `X` e `w`.\n",
    "    - A operação de produto interno é realizada por meio da função externa `dot`.\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = []\n",
    "\n",
    "    for x_i in X:\n",
    "        y_pred.append(dot(w, x_i) + b)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaac944",
   "metadata": {},
   "source": [
    "### 3. *Loss Function*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87a9fcd",
   "metadata": {},
   "source": [
    "A verossimilhança das predições do modelo é avaliada por meio do Erro Quadrático Médio (*Mean Squared Error – MSE*), definido como:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{1}{N}  \\sum_{i=1}^{N} \\left(y_i - \\hat{y}_i\\right)^2\n",
    "$$\n",
    "\n",
    "Em que:\n",
    "- $\\mathbf{y} \\in \\mathbb{R}^{N}$ é o vetor de valores verdadeiros (*targets*);\n",
    "- $\\hat{\\mathbf{y}} \\in \\mathbb{R}^{N}$ é o vetor de predições do modelo;\n",
    "- $N$ é o número total de amostras.\n",
    "\n",
    "O MSE penaliza erros grandes de forma quadrática, tornando-se particularmente sensível a desvios significativos entre predição e valor real.\n",
    "\n",
    "\n",
    "**Observações Conceituais**\n",
    "- A função de perda é convexa em relação a $\\hat{\\mathbf{y}}$ e, por composição, em relação aos parâmetros $\\mathbf{w}$ e $b$ de modelos lineares.\n",
    "- É diferenciável em todo o domínio, o que a torna apropriada para métodos de otimização baseados em gradiente.\n",
    "- Para Ruído Gaussiano Aditivo, a minimização do MSE equivale à estimação de máxima verossimilhança dos parâmetros do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "92178735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula a função de perda Erro Quadrático Médio (*Mean Squared Error – MSE*).\n",
    "\n",
    "    Dado um conjunto de valores verdadeiros e suas respectivas predições, a função\n",
    "    computa a perda definida por\n",
    "\n",
    "        L(y, ŷ) = (1 / N) ∑_{i=1}^{N} (y_i − ŷ_i)²,\n",
    "\n",
    "    onde y_i ∈ ℝ representa o valor alvo (*ground truth*), ŷ_i ∈ ℝ a predição do modelo\n",
    "    e N o número total de amostras. O MSE corresponde à média empírica do erro quadrático.\n",
    "\n",
    "    ### Parâmetros\n",
    "    - `y_true` (list[float]):\n",
    "        Vetor contendo os valores alvo verdadeiros, y = (y₁, y₂, …, y_N).\n",
    "\n",
    "    - `y_pred` (list[float]):\n",
    "        Vetor contendo as predições do modelo, ŷ = (ŷ₁, ŷ₂, …, ŷ_N).\n",
    "\n",
    "    ### Retorno\n",
    "    float\n",
    "\n",
    "        Valor escalar correspondente à perda MSE computada sobre todo o conjunto\n",
    "        de amostras.\n",
    "\n",
    "    ### Observações\n",
    "    - Assume-se correspondência um-para-um entre `y_true` e `y_pred`.\n",
    "    - A função retorna a perda média, e não a soma dos erros quadráticos.\n",
    "    - Não há normalização adicional nem ponderação por amostra.\n",
    "    - A função é diferenciável em relação a `y_pred`, o que a torna adequada\n",
    "      para otimização baseada em gradiente.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assegura dimensionalidade equivalente entre y_true e y_pred \n",
    "    assert len(y_true) == len(y_pred)\n",
    "\n",
    "    total_err = 0\n",
    "    N = len(y_true)\n",
    "\n",
    "    for i in range(N):\n",
    "        total_err += (y_true[i] - y_pred[i])**2\n",
    "    \n",
    "    loss = total_err / N\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed48ab5a",
   "metadata": {},
   "source": [
    "### 4. *Backward Pass*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d04e3a7",
   "metadata": {},
   "source": [
    "O *backward pass* do modelo linear consiste na avaliação explícita dos gradientes da função de perda em relação aos parâmetros treináveis. \n",
    "\n",
    "Considerando o modelo:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\mathbf{w}^{\\top}\\mathbf{x}_i + b\n",
    "$$\n",
    "\n",
    "E a função de perda Erro Quadrático Médio:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\left(y_i - \\hat{y}_i\\right)^2\n",
    "$$\n",
    "\n",
    "Os gradientes analíticos são dados por:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} \\mathcal{L} = \\frac{2}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i),\\mathbf{x}_i, \\qquad \\frac{\\partial \\mathcal{L}}{\\partial b} = \n",
    "\\frac{2}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i).\n",
    "$$\n",
    "\n",
    "Essas expressões resultam diretamente da aplicação da regra da cadeia e constituem a base para métodos de otimização por gradiente descendente.\n",
    "\n",
    "**Observações Conceituais**\n",
    "- Cada termo $(\\hat{y}_i - y_i)$ representa o erro de predição da $i$-ésima amostra.\n",
    "- O gradiente em relação aos pesos é uma soma ponderada dos vetores de entrada, escalada pelo erro.\n",
    "- O gradiente em relação ao viés corresponde à média dos erros, refletindo um deslocamento global da função afim.\n",
    "- A normalização por $N$ garante que a magnitude do gradiente seja independente do tamanho do conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "2c7f05d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computa o *backward pass* de um modelo linear, avaliando analiticamente\n",
    "    os gradientes da função de perda MSE em relação aos parâmetros do modelo.\n",
    "\n",
    "    Considerando um modelo linear da forma\n",
    "\n",
    "        ŷ_i = wᵀ x_i + b,\n",
    "\n",
    "    e a função de perda Erro Quadrático Médio\n",
    "\n",
    "        L = (1 / N) ∑_{i=1}^{N} (y_i − ŷ_i)²,\n",
    "\n",
    "    a função calcula os gradientes\n",
    "\n",
    "     ∂L/∂w = (2 / N) ∑_{i=1}^{N} (ŷ_i − y_i) x_i\n",
    "        \n",
    "     ∂L/∂b = (2 / N) ∑_{i=1}^{N} (ŷ_i − y_i),\n",
    "\n",
    "    os quais podem ser utilizados em um passo subsequente de atualização\n",
    "    dos parâmetros via métodos de otimização baseados em gradiente.\n",
    "\n",
    "    ### Parâmetros\n",
    "    - `X` (list[list[float]]): Conjunto de N vetores de entrada, X = {x₁, x₂, …, x_N}, com x_i ∈ ℝᵈ.\n",
    "\n",
    "    - `y_true` (list[float]): Vetor dos valores alvo verdadeiros, y = (y₁, y₂, …, y_N).\n",
    "\n",
    "    - `y_pred` (list[float]): Vetor das predições do modelo, ŷ = (ŷ₁, ŷ₂, …, ŷ_N), obtidas no *forward pass*.\n",
    "\n",
    "    ### Retorno\n",
    "    tuple[list[float], float]\n",
    "\n",
    "        Tupla `(grad_w, grad_b)` contendo:\n",
    "\n",
    "    - `grad_w`: Vetor dos gradientes da perda em relação aos pesos, ∂L/∂w ∈ ℝᵈ;\n",
    "\n",
    "    - `grad_b`: Escalar correspondente ao gradiente da perda em relação ao viés, ∂L/∂b ∈ ℝ.\n",
    "\n",
    "    ### Observações\n",
    "    - Os gradientes são calculados de forma explícita (analítica), sem o uso\n",
    "      de diferenciação automática.\n",
    "    - Assume-se correspondência dimensional entre `X`, `w` e `y_pred`.\n",
    "    - A função não realiza a atualização dos parâmetros, apenas computa os gradientes.\n",
    "    - O fator de normalização 1/N está embutido no cálculo final dos gradientes.\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(X)\n",
    "    d = len(X[0])\n",
    "\n",
    "    grad_w = [0.0 for _ in range(d)]\n",
    "    grad_b = 0.0\n",
    "\n",
    "    for i in range(N):\n",
    "        error = y_true[i] - y_pred[i]\n",
    "\n",
    "        for j in range(d):\n",
    "            grad_w[j] += -2 * error * X[i][j]\n",
    "\n",
    "        grad_b += -2 * error\n",
    "\n",
    "    grad_w = [grad_w[k] / N for k in range(d)]\n",
    "    grad_b /= N\n",
    "\n",
    "    return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddf40d0",
   "metadata": {},
   "source": [
    "### 5. *Gradient Descent*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e0db5",
   "metadata": {},
   "source": [
    "A etapa de otimização do modelo é realizada por meio do Gradiente Descendente, que atualiza iterativamente os parâmetros treináveis na direção oposta ao gradiente da função de perda. Para um modelo linear, a regra de atualização é dada por:\n",
    "$$\n",
    "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta , \\nabla_{\\mathbf{w}} \\mathcal{L}, \\qquad b^{(t+1)} = b^{(t)} - \\eta , \\frac{\\partial \\mathcal{L}}{\\partial b}\n",
    "$$\n",
    "\n",
    "Em que:\n",
    "- $\\eta > 0$ é a taxa de aprendizado (*learning rate*);\n",
    "- $\\nabla_{\\mathbf{w}} \\mathcal{L}$ e $\\frac{\\partial \\mathcal{L}}{\\partial b}$ são os gradientes computados no *backward pass*;\n",
    "- $t$ denota a iteração de treinamento.\n",
    "\n",
    "Essa atualização corresponde a um único passo de descida no espaço de parâmetros, com o objetivo de minimizar a função de perda.\n",
    "\n",
    "**Observações Conceituais**\n",
    "- O Gradiente Descendente explora o fato de que, em modelos lineares com MSE, a função de perda é convexa, assegurando convergência ao mínimo global sob escolha adequada de $\\eta$.\n",
    "- A magnitude da taxa de aprendizado controla o compromisso entre velocidade de convergência e estabilidade numérica.\n",
    "- A ausência de *momentum* ou mecanismos adaptativos torna essa implementação conceitualmente transparente e didaticamente valiosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "8da82bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(w, b, grad_w, grad_b, lr):\n",
    "    \"\"\"\n",
    "    Atualiza os parâmetros de um modelo linear por meio do método de\n",
    "    Gradiente Descendente (*Gradient Descent*).\n",
    "\n",
    "    Dado um conjunto de parâmetros treináveis e seus respectivos gradientes\n",
    "    em relação a uma função de perda L, a função executa a atualização explícita\n",
    "\n",
    "     w ← w − η ∂L/∂w\n",
    "\n",
    "     b ← b − η ∂L/∂b,\n",
    "\n",
    "    onde η > 0 denota a taxa de aprendizado (*learning rate*). Essa etapa\n",
    "    corresponde ao passo de otimização subsequente ao *backward pass*.\n",
    "\n",
    "    ### Parâmetros\n",
    "    - `w` (list[float]): Vetor de pesos do modelo, w ∈ ℝᵈ.\n",
    "\n",
    "    - `b` (float): Escalar correspondente ao termo de viés do modelo.\n",
    "\n",
    "    - `grad_w` (list[float]): Vetor dos gradientes da função de perda em relação aos pesos, ∂L/∂w ∈ ℝᵈ.\n",
    "\n",
    "    - `grad_b` (float): Gradiente da função de perda em relação ao viés, ∂L/∂b ∈ ℝ.\n",
    "\n",
    "    - `lr` (float): Taxa de aprendizado η, que controla o tamanho do passo de atualização no espaço de parâmetros.\n",
    "\n",
    "    ### Retorno\n",
    "    tuple[list[float], float]\n",
    "\n",
    "        Tupla `(w, b)` contendo os parâmetros atualizados após um passo de aplcação do Gradiente Descendente.\n",
    "\n",
    "    ### Observações\n",
    "    - A atualização é realizada *in-place* para o vetor de pesos `w`.\n",
    "    - A função implementa Gradiente Descendente em sua forma mais simples,\n",
    "      sem *momentum*, regularização ou adaptação da taxa de aprendizado.\n",
    "    - Assume-se compatibilidade dimensional entre `w` e `grad_w`.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(len(grad_w)):\n",
    "      w[i] = w[i] - lr * grad_w[i]\n",
    "        \n",
    "    b = b - lr * grad_b\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea79bfe",
   "metadata": {},
   "source": [
    "### 6. *Training Loop*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9e4411",
   "metadata": {},
   "source": [
    "O treinamento do modelo é realizado por meio de um *loop* iterativo de otimização baseada em gradiente, no qual os parâmetros do modelo são ajustados progressivamente para minimizar a função de perda Erro Quadrático Médio (MSE):\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w}, b) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(y_i - (\\mathbf{w}^{\\top}\\mathbf{x}_i + b)\\right)^2.\n",
    "$$\n",
    "\n",
    "A cada época, o seguinte ciclo é executado:\n",
    "\n",
    "1. Inicialização dos parâmetros $(\\mathbf{w}, b)$ (realizada uma única vez);\n",
    "2. *Forward pass* para obtenção das predições $\\hat{\\mathbf{y}}$;\n",
    "3. Avaliação da perda $\\mathcal{L}$;\n",
    "4. *Backward pass* para cálculo analítico dos gradientes;\n",
    "5. Atualização dos parâmetros via Gradiente Descendente.\n",
    "\n",
    "De forma abstrata, o processo pode ser descrito como:\n",
    "\n",
    "$$\n",
    "(\\mathbf{w}, b) \\xleftarrow{\\text{iterativamente}} \\arg\\min_{\\mathbf{w}, b} \\mathcal{L}(\\mathbf{w}, b), \\quad \\text{via Gradiente Descendente em modo batch}.\n",
    "$$\n",
    "\n",
    "O vetor `loss_log` armazena o valor da função de perda ao final de cada época, permitindo a análise da dinâmica de convergência do treinamento.\n",
    "\n",
    "**Observações conceituais**\n",
    "- O treinamento é realizado em modo *batch*, utilizando todo o conjunto de dados em cada iteração.\n",
    "- Em modelos lineares com MSE, a função objetivo é convexa, garantindo convergência ao mínimo global sob escolha adequada da taxa de aprendizado.\n",
    "- A ausência de *early stopping*, regularização ou validação torna o procedimento conceitualmente simples e transparente, favorecendo análise didática do comportamento do gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "f232d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, epochs, lr):\n",
    "    \"\"\"\n",
    "    Executa o procedimento de treinamento de um modelo linear por otimização iterativa baseada em gradiente.\n",
    "\n",
    "    A função implementa um *training loop* completo, composto pelas etapas\n",
    "    de inicialização dos parâmetros, *forward pass*, avaliação da função de\n",
    "    perda Erro Quadrático Médio (MSE), *backward pass* para cálculo dos gradientes\n",
    "    e atualização dos parâmetros via Gradiente Descendente.\n",
    "\n",
    "    Em cada época, o modelo ajusta seus parâmetros `(w, b)` de modo a minimizar\n",
    "    a função objetivo\n",
    "\n",
    "        L(w, b) = (1 / N) ∑_{i=1}^{N} (y_i − (wᵀ x_i + b))².\n",
    "\n",
    "    ### Parâmetros\n",
    "    - `X` (list[list[float]]): Conjunto de N vetores de entrada, X = {x₁, x₂, …, x_N}, com x_i ∈ ℝᵈ.\n",
    "\n",
    "    - `y` (list[float]): Vetor dos valores alvo associados às instâncias de entrada, y = (y₁, y₂, …, y_N).\n",
    "\n",
    "    - `epochs` (int): Número de iterações completas sobre o conjunto de dados (épocas de treinamento).\n",
    "\n",
    "    - `lr` (float): Taxa de aprendizado utilizada no método de Gradiente Descendente.\n",
    "\n",
    "    ### Retorno\n",
    "    tuple\n",
    "\n",
    "     Tupla `(w, b, loss_log)` contendo:\n",
    "     - `w` (list): Vetor contendo os pesos finais estimados.\n",
    "     - `b` (float): Escalar representando o viés final estimado.\n",
    "     - `loss_log` (list): Registro da perda do modelo ao fim de cada época de treino.\n",
    "  \n",
    "\n",
    "    ### Observações\n",
    "    - O treinamento é realizado em modo *batch*, utilizando todo o conjunto\n",
    "      de dados a cada época.\n",
    "    - Os parâmetros do modelo são inicializados uma única vez, antes do início\n",
    "      do loop de treinamento.\n",
    "    - A função não retorna os parâmetros finais `(w, b)`; apenas o histórico\n",
    "      da perda pode ser recuperado opcionalmente.\n",
    "    - Não há critérios de parada antecipada (*early stopping*), validação\n",
    "      ou regularização.\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "    loss_log = []\n",
    "\n",
    "    n_features = len(X[0])\n",
    "    w, b = model(n_features)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = forward(X, w, b)\n",
    "        loss = mse(y, y_pred)\n",
    "        loss_log.append(loss)\n",
    "        grad_w, grad_b = backward(X, y, y_pred)\n",
    "        w, b = gradient_descent(w, b, grad_w, grad_b, lr)\n",
    "\n",
    "    return w, b, loss_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15368b4f",
   "metadata": {},
   "source": [
    "# Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "b050c344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (MSE) final do modelo: 0.9828628897967788\n"
     ]
    }
   ],
   "source": [
    "# Geração de dados sintéticos\n",
    "n_examples, n_features = (1_000, 5)\n",
    "X, y, w_true, b_true = synth_data(n_examples, n_features, return_params=True)\n",
    "\n",
    "# Treino do modelo\n",
    "w, b, loss_log = train(X, y, epochs=10_000, lr=0.001)\n",
    "\n",
    "print(f'Loss (MSE) final do modelo: {loss_log[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0308342d",
   "metadata": {},
   "source": [
    "# Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3e61d7",
   "metadata": {},
   "source": [
    "Comparação entre os parâmetros fundamentalmente verdadeiros definidos na geração dos dados sintéticos e os parâmetros estimados pelo modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "23d89f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parâmetros verdadeiros:\n",
      "w_true = [-0.5183735347655064, 0.3892625131435465, 1.1464241141605414, -0.8063735255551222, -1.5231055130747249]\n",
      "b_true = 8.0\n",
      "\n",
      "Parâmetros aprendidos:\n",
      "w_hat  = [-0.5130019754214008, 0.3701679696990196, 1.1335186935614157, -0.8121331362295992, -1.529172872508083]\n",
      "b_hat  = 7.96790247576778\n"
     ]
    }
   ],
   "source": [
    "print(\"Parâmetros verdadeiros:\")\n",
    "print(\"w_true =\", w_true)\n",
    "print(\"b_true =\", b_true)\n",
    "\n",
    "print(\"\\nParâmetros aprendidos:\")\n",
    "print(\"w_hat  =\", w)\n",
    "print(\"b_hat  =\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45bb1a9",
   "metadata": {},
   "source": [
    "Visualização da evolução da *loss* do modelo ao longo das épocas de treino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "cb4eb3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(loss_log)\n",
    "\n",
    "ax.set_title('$Loss$ do Modelo ao Longo das Épocas')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_xlabel('Época');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ddf06b",
   "metadata": {},
   "source": [
    "# Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90203c",
   "metadata": {},
   "source": [
    "Os resultados obtidos para a Rede Neural Regressora Linear de Camada Única Integralmente Conectada, implementada integralmente em Python puro, evidenciam convergência adequada, estabilidade numérica e consistência teórico-matemática do modelo e do procedimento de treinamento adotado.\n",
    "\n",
    "Do ponto de vista estrutural, o modelo em análise corresponde a uma rede neural linear do tipo *single-layer fully connected*, cuja função hipótese pode ser expressa como:\n",
    "\n",
    "$$\n",
    "\\hat{y}(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "Portanto, sendo matematicamente equivalente ao modelo clássico de regressão linear multivariada. Nessa configuração, a superfície de erro associada é a função de perda quadrática média (MSE):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w}, b) = \\frac{1}{N}\\sum_{i=1}^N \\left(y_i - (\\mathbf{w}^\\top \\mathbf{x}_i + b)\\right)^2\n",
    "$$\n",
    "\n",
    "Isso assegura convexidade, garantindo a existência de um único mínimo global. Assim, sob condições adequadas de taxa de aprendizado e inicialização, o gradiente descendente deve convergir para a solução ótima — fato corroborado empiricamente pelos resultados apresentados.\n",
    "\n",
    "**Análise da convergência do treinamento**\n",
    "\n",
    "A curva de *loss* ao longo das épocas revela um comportamento típico de sistemas convexos bem condicionados: uma queda acentuada nas iterações iniciais, seguida por uma região de saturação assintótica, na qual o erro se estabiliza próximo ao mínimo global. A ausência de oscilações ou instabilidades indica que:\n",
    "- O cálculo analítico dos gradientes está correto;\n",
    "- A taxa de aprendizado encontra-se em regime estável;\n",
    "- Não há problemas de explosão ou desaparecimento do gradiente, como esperado para modelos lineares rasos.\n",
    "\n",
    "Esse comportamento reforça a correção tanto da formulação matemática quanto da implementação algorítmica do treinamento.\n",
    "\n",
    "**Qualidade da estimação paramétrica**\n",
    "\n",
    "A comparação entre os parâmetros verdadeiros e os parâmetros aprendidos mostra erro relativo baixo, tanto nos pesos quanto no viés:\n",
    "\n",
    "- Pesos verdadeiros:\n",
    "  $$\n",
    "  \\mathbf{w}_{\\text{true}} = (-1.5231, 1.1464)\n",
    "  $$\n",
    "\n",
    "- Pesos aprendidos:\n",
    "  $$\n",
    "  \\mathbf{w}_{\\text{hat}} = (-1.5292, 1.1335)\n",
    "  $$\n",
    "\n",
    "- Viés verdadeiro:\n",
    "  $$\n",
    "  b_{\\text{true}} = 8.0\n",
    "  $$\n",
    "\n",
    "- Viés aprendido:\n",
    "  $$\n",
    "  b_{\\text{hat}} \\approx 7.967\n",
    "  $$\n",
    "\n",
    "As discrepâncias observadas são pequenas e estatisticamente compatíveis com a presença de ruído nos dados e com o fato de o treinamento ser realizado por otimização iterativa, e não por solução analítica fechada (equações normais). Em particular, a proximidade entre $\\mathbf{w}_{\\text{true}}$ e $\\mathbf{w}_{\\text{hat}}$ indica que o modelo foi capaz de recuperar corretamente a estrutura linear subjacente ao processo gerador dos dados.\n",
    "\n",
    "**Interpretação do valor final da perda**\n",
    "\n",
    "O valor final da função de perda,\n",
    "\n",
    "$$\n",
    "\\text{MSE} \\approx 0.98,\n",
    "$$\n",
    "\n",
    "implica um erro quadrático médio da ordem da unidade. Considerando que o modelo é linear e que os dados contêm ruído aditivo, esse valor é coerente com um cenário em que:\n",
    "- A capacidade do modelo está adequadamente alinhada à complexidade do problema;\n",
    "- Não há *overfitting*, dado o caráter simples da hipótese;\n",
    "- O erro residual representa majoritariamente variância irredutível dos dados.\n",
    "\n",
    "Em termos de *Deep Learning*, isso caracteriza um regime de viés estrutural controlado, com variância limitada, exatamente o comportamento esperado para uma rede linear treinada sob MSE.\n",
    "\n",
    "**Considerações finais**\n",
    "\n",
    "Em síntese, os resultados obtidos validam plenamente:\n",
    "\n",
    "1. a formulação matemática da rede neural linear;\n",
    "2. a implementação manual do *forward pass*, do *backpropagation* e da atualização dos parâmetros;\n",
    "3. a escolha da função de perda e do método de otimização.\n",
    "\n",
    "Além disso, este experimento constitui uma base sólida e conceitualmente correta para extensões futuras, tais como:\n",
    "- Introdução de camadas ocultas e não linearidades;\n",
    "- Análise comparativa entre otimização analítica e gradiente descendente;\n",
    "- Estudo do impacto de regularização $\\ell_2$ e $\\ell_1$;\n",
    "- Generalização para problemas de regressão não linear.\n",
    "\n",
    "Portanto, conclui-se que o modelo não apenas converge corretamente, mas também materializa, de forma didática e rigorosa, a ponte entre regressão linear clássica e redes neurais no contexto moderno de aprendizado profundo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
