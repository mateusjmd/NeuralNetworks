{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab252ea",
   "metadata": {},
   "source": [
    "Neural Network: Multi Layer Perceptron\n",
    "===\n",
    "\n",
    "**Autor:** Mateus de Jesus Mendes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6fb8f4",
   "metadata": {},
   "source": [
    "### Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb37b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eb1d76",
   "metadata": {},
   "source": [
    "### Definições Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e551a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 88\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483a97d1",
   "metadata": {},
   "source": [
    "### Funções Auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc51f5de",
   "metadata": {},
   "source": [
    "Funções secundárias utilizadas nas etapas principais da metodologia. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef4076",
   "metadata": {},
   "source": [
    "##### **Produto Interno**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(a, b):\n",
    "    \"\"\"\n",
    "    Cálculo do produto interno dos casos:\n",
    "    - Vetor-vetor\n",
    "    - Matriz-vetor\n",
    "    - Matriz-matriz\n",
    "    \n",
    "    ### Parâmetros\n",
    "    - `a`/`b`: Vetor ou matriz.\n",
    "\n",
    "    ### Retorna\n",
    "    Float (escalar) para produto vetor-vetor / Lista (vetor) para produto matriz-vetor / Matriz (lista de listas) para produto matriz-matriz.\n",
    "    \"\"\"\n",
    "\n",
    "    # Funções auxiliares de identificação\n",
    "    def is_vector(x):\n",
    "        return isinstance(x[0], (int, float))\n",
    "\n",
    "    def is_matrix(x):\n",
    "        return isinstance(x[0], list)\n",
    "\n",
    "    # Produto vetor-vetor -> escalar\n",
    "    if is_vector(a) and is_vector(b):\n",
    "        if len(a) != len(b):\n",
    "            raise ValueError(\"Vetores devem ter o mesmo comprimento.\")\n",
    "        \n",
    "        s = 0\n",
    "        for i in range(len(a)):\n",
    "            s += a[i] * b[i]\n",
    "        return s\n",
    "\n",
    "    # Produto matriz-vetor -> vetor\n",
    "    if is_matrix(a) and is_vector(b):\n",
    "        n_rows = len(a)\n",
    "        n_cols = len(a[0])\n",
    "\n",
    "        if n_cols != len(b):\n",
    "            raise ValueError(\"Dimensões incompatíveis para produto matriz-vetor.\")\n",
    "\n",
    "        result = []\n",
    "        for i in range(n_rows):\n",
    "            sum = 0\n",
    "            for j in range(n_cols):\n",
    "                sum += a[i][j] * b[j]\n",
    "            result.append(sum)\n",
    "        return result\n",
    "\n",
    "    # Produto matriz-matriz -> matriz\n",
    "    if is_matrix(a) and is_matrix(b):\n",
    "        n_rows_a = len(a)\n",
    "        n_cols_a = len(a[0])\n",
    "        n_rows_b = len(b)\n",
    "        n_cols_b = len(b[0])\n",
    "\n",
    "        if n_cols_a != n_rows_b:\n",
    "            raise ValueError(\"Dimensões incompatíveis para produto matriz-matriz.\")\n",
    "\n",
    "        result = []\n",
    "        for i in range(n_rows_a):\n",
    "            row = []\n",
    "            for j in range(n_cols_b):\n",
    "                sum = 0\n",
    "                for k in range(n_cols_a):\n",
    "                    sum += a[i][k] * b[k][j]\n",
    "                row.append(sum)\n",
    "            result.append(row)\n",
    "        return result\n",
    "\n",
    "    # Caso inválido\n",
    "    raise TypeError(\"Entradas devem ser vetores ou matrizes válidas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2e4c1b",
   "metadata": {},
   "source": [
    "##### ***Holdout***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb0916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout(X, y, train_size=0.8):\n",
    "    \"\"\"\n",
    "    Realiza a partição *holdout* de um conjunto de dados supervisionado\n",
    "    em subconjuntos de treinamento e teste.\n",
    "\n",
    "    A função separa aleatoriamente o conjunto de dados original\n",
    "    D = {(x_i, y_i)}_{i=1}^{N} em dois subconjuntos disjuntos:\n",
    "    um conjunto de treinamento D_train e um conjunto de teste D_test,\n",
    "    respeitando a proporção especificada por `train_size`.\n",
    "\n",
    "    ### Parâmetros\n",
    "    - `X` (list[list[float]]): Conjunto de N vetores de entrada, X = {x₁, x₂, …, x_N}, com x_i ∈ ℝᵈ.\n",
    "\n",
    "    - `y` (list[float]): Vetor dos valores alvo associados às instâncias de entrada, y = (y₁, y₂, …, y_N).\n",
    "\n",
    "    - `train_size` (float, opcional): Fração do conjunto de dados destinada ao treinamento, com 0 < `train_size` < 1.\n",
    "\n",
    "    ### Retorno\n",
    "    tuple[list, list, list, list]\n",
    "\n",
    "        Tupla `(X_train, X_test, y_train, y_test)`, onde:\n",
    "     - `X_train`: subconjunto dos vetores de entrada utilizados no treinamento;\n",
    "     - `X_test`: subconjunto dos vetores de entrada utilizados na avaliação;\n",
    "     - `y_train`: valores alvo correspondentes ao conjunto de treinamento;\n",
    "     - `y_test`: valores alvo correspondentes ao conjunto de teste.\n",
    "\n",
    "    ### Observações\n",
    "    - A partição é realizada por embaralhamento aleatório dos índices,\n",
    "      caracterizando um *holdout* simples.\n",
    "    - Não há estratificação em relação à variável alvo.\n",
    "    - Não há controle explícito de *seed*, o que implica não reprodutibilidade\n",
    "      entre execuções distintas.\n",
    "    - A função preserva a correspondência entre instâncias de entrada e seus\n",
    "      respectivos valores alvo.\n",
    "    \"\"\"\n",
    "\n",
    "    if not 0 < train_size < 1:\n",
    "        raise ValueError('train_size deve estar no intervalo (0, 1).')\n",
    "    \n",
    "    if len(X) != len(y):\n",
    "        raise ValueError('X e y devem ter o mesmo comprimento.')\n",
    "\n",
    "    n = len(X)\n",
    "    indices = list(range(n))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    split = int(n * train_size)\n",
    "\n",
    "    train_idx = indices[:split]\n",
    "    test_idx = indices[split:]\n",
    "\n",
    "    X_train = [X[i] for i in train_idx]\n",
    "    y_train = [y[i] for i in train_idx]\n",
    "\n",
    "    X_test = [X[i] for i in test_idx]\n",
    "    y_test = [y[i] for i in test_idx]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d74651",
   "metadata": {},
   "source": [
    "### Geração / Leitura de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88756ba6",
   "metadata": {},
   "source": [
    "O conjunto de dados é gerado a partir do modelo linear com ruído aditivo:\n",
    "$$\n",
    "y_i = \\mathbf{w}_{\\text{true}}^{\\top}\\mathbf{x}_i + b_{\\text{true}} + \\varepsilon_i,\n",
    "\\quad i = 1, \\dots, N\n",
    "$$\n",
    "\n",
    "Em que:\n",
    "- $\\mathbf{x}_i \\in \\mathbb{R}^d$: Vvetor de *features* da $i$-ésima amostra.\n",
    "- $\\mathbf{w}_{\\text{true}} \\in \\mathbb{R}^d$: Vetor de pesos verdadeiro.\n",
    "- $b_{\\text{true}} \\in \\mathbb{R}$: Viés verdadeiro.\n",
    "- $\\varepsilon_i \\sim \\mathcal{N}(0, 1)$: Ruído Gaussiano Aditivo Independente.\n",
    "\n",
    "Os vetores $\\mathbf{x}_i$ também são amostrados de uma Distribuição Normal Padrão:\n",
    "$$\n",
    "\\mathbf{x}_i \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab13bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticLinearData:\n",
    "    def __init__(self, n_samples, n_features):\n",
    "        self.n_samples = n_samples\n",
    "        self.n_features = n_features\n",
    "    \n",
    "    def generate(self):\n",
    "        self.w_true = [random.gauss() for _ in range(self.n_features)]\n",
    "        self.b_true = random.uniform(0, 9)\n",
    "\n",
    "        X, y = [], []\n",
    "\n",
    "        for _ in range(self.n_samples):\n",
    "            x_i = [random.gauss() for _ in range(self.n_features)]\n",
    "            noise = random.gauss()\n",
    "            y_i = dot(self.w_true, x_i) + self.b_true + noise\n",
    "\n",
    "            X.append(x_i)\n",
    "            y.append(y_i)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d528846d",
   "metadata": {},
   "source": [
    "### Definição do Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3bbfd1",
   "metadata": {},
   "source": [
    "Classe base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d86237",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def forward(self, X):\n",
    "        # Subclasses implementam o método forward\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # Subclasses implementam o método backward\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c32161",
   "metadata": {},
   "source": [
    "*Fully Connected Layer*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1817c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        # Inicializa os pesos com valores aleatórios centrados em 0\n",
    "        self.W = [\n",
    "            [random.uniform(-1e-3, 1e-3) for _ in range(in_features)] for _ in range(out_features)\n",
    "            ]\n",
    "        \n",
    "        # Inicializa os vieses como zeros\n",
    "        self.b = [0.0] * out_features\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Armazena a entrada para uso posterior no Bacwarkd Pass\n",
    "        self.X = X\n",
    "\n",
    "        # Transformação afim\n",
    "        return [ [dot(self.W[i], x_i) + self.b[i] for i in range(len(self.W))] for x_i in X]\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # Número de amostras no batch\n",
    "        N = len(self.X)\n",
    "\n",
    "        # Dimensão de entrada (número de features)\n",
    "        in_dim = len(self.W[0])\n",
    "        # Dimensão de saída (número de neurônios)\n",
    "        out_dim = len(self.W)\n",
    "\n",
    "        # Acumula os gradientes dos pesos, vieses e entradas\n",
    "        grad_W = [[0.0] * in_dim for _ in range(out_dim)]\n",
    "        grad_b = [0.0] * out_dim\n",
    "        grad_input = [[0.0] * in_dim for _ in range(N)]\n",
    "\n",
    "        # Loop sobre o batch, neurônios de saída e componentes de entrada\n",
    "        for i in range(N): # Percorre as amostras\n",
    "            for j in range(out_dim): # Percorre os neurônios de saída\n",
    "                # Gradiente do viés\n",
    "                grad_b[j] += grad_output[i][j]\n",
    "\n",
    "                for k in range(in_dim): # Percorre as features de entrada\n",
    "                    # ∂L/∂W_jk = ∂L/∂y_j * x_k\n",
    "                    grad_W[j][k] += grad_output[i][j] * self.X[i][k]\n",
    "                    \n",
    "                    # ∂L/∂x_k = Σ_j (∂L/∂y_j * W_jk)\n",
    "                    grad_input[i][k] == grad_output[i][j] * self.W[j][k]\n",
    "        \n",
    "        # Armazena os gradientes para uso pelo otimizador\n",
    "        self.grad_W = grad_W\n",
    "        self.grad_b = grad_b\n",
    "\n",
    "        # Retorna o gradiente em relação à entrada da camada\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f87cb",
   "metadata": {},
   "source": [
    "Função de ativação $\\mathrm{ReLU}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def forward(self, X):\n",
    "        # Armazena a entrada para uso posterior no Backward Pass\n",
    "        self.X = X\n",
    "\n",
    "        # Aplica a função ReLU element-wise\n",
    "        return [[max(0, x_i) for x_i in sample] for sample in X]\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # Para cada elemento do batch:\n",
    "        # - Se a entrada original foi positiva, o gradiente é preservado\n",
    "        # - Senão, o gradiente é anulado\n",
    "        return [ [grad_output[i][j] if self.X[i][j] else 0.0 for j in range(len(self.X[0]))] for i in range(len(self.X))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06d2b82",
   "metadata": {},
   "source": [
    "Composição de camadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70db85e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layers):\n",
    "        # Armazena a arquitetura da rede como uma lista de camadas\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Propaga os dados camada a camada, atualizando X a cada etapa\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "\n",
    "        # Ao fim do loop, X contém a saída da última camada\n",
    "        return X\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # Percorre as camadas em ordem reversa, propagando os gradientes\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d9d84",
   "metadata": {},
   "source": [
    "*Loss* (MSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d113edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "      \t# Armazena predições e rótulos verdadeiros para uso no cálculo do gradiente \n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "\n",
    "        # Número de amostras no batch\n",
    "        N = len(y_true)\n",
    "\n",
    "        # Calcula o Erro Quadrático Médio (MSE)\n",
    "        return sum((y_pred[i][0] - y_true[i])**2 for i in range(N)) / N\n",
    "    \n",
    "    def backward(self):\n",
    "      \t# Número de amostras no batch\n",
    "        N = len(self.y_true)\n",
    "\n",
    "        # Para cada amostra, calcula o gradiente escalar e o encapsula numa lista\n",
    "        return [ [2 * (self.y_pred[i][0] - self.y_true[i]) / N] for i in range(N) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a321fd91",
   "metadata": {},
   "source": [
    "Otimizador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4f7f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr):\n",
    "      \t# Armazena a taxa de aprendizado\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, model):\n",
    "      \t# Itera sobre todas as camadas do modelo\n",
    "        for layer in model.layers:\n",
    "          \t            \n",
    "            if hasattr(layer, 'W'): # Verifica se a camada tem pesos treináveis\n",
    "              \n",
    "              \t# Para cada elemento da matriz W aplica-se o Gradiente Descendente\n",
    "                for i in range(len(layer.W)):\n",
    "                    for j in range(len(layer.W[0])):\n",
    "                        layer.W[i][j] -= self.lr * layer.grad_W[i][j]\n",
    "                \n",
    "                # Cada componente do vetor b é atualizado analogamente aos pesos\n",
    "                for i in range(len(layer.b)):\n",
    "                    layer.b[i] -= self.lr * layer.grad_b[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd63a0",
   "metadata": {},
   "source": [
    "*Loop* de treino genérico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6e7bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geração de conjunto de dados sintético\n",
    "data = SyntheticLinearData(100, 5)\n",
    "X, y = data.generate()\n",
    "\n",
    "# Divisão em subconjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = holdout(X, y)\n",
    "model = MLP([\n",
    "    Linear(5, 100),\n",
    "    ReLU(),\n",
    "    Linear(100, 1)\n",
    "])\n",
    "\n",
    "# Definição da Loss Function\n",
    "loss_fn = MSELoss()\n",
    "\n",
    "# Definição do otimizador\n",
    "optimizer = SGD(lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(10_000):\n",
    "  \n",
    "  \t# Forward Pass\n",
    "    y_pred = model.forward(X_train)\n",
    "    \n",
    "    # Cálculo da Loss Function\n",
    "    loss = loss_fn.forward(y_pred, y_train)\n",
    "\n",
    "    # Backward Pass da Loss Function\n",
    "    grad_loss = loss_fn.backward()\n",
    "    # Backward Pass do modelo\n",
    "    model.backward(grad_loss)\n",
    "\n",
    "    # Otimização dos parâmetros\n",
    "    optimizer.step(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
