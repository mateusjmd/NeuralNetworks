{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b222201",
   "metadata": {},
   "source": [
    "Neural Network: Linear Regression\n",
    "===\n",
    "\n",
    "**Autor:** Mateus de Jesus Mendes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7bb50d",
   "metadata": {},
   "source": [
    "# Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17216e6c",
   "metadata": {},
   "source": [
    "O presente *Jupyter Notebook* tem por objetivo implementar em Python puro uma Rede Neural Linear de Camada Única Integralmente Conectada para regressão utilizando Programação Orientada a Objetos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72ddcb",
   "metadata": {},
   "source": [
    "# Metodologia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e026c4e4",
   "metadata": {},
   "source": [
    "### Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "9336a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b03a95",
   "metadata": {},
   "source": [
    "### Definições Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "bd90381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 88\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc8050",
   "metadata": {},
   "source": [
    "### Funções Auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf139ff",
   "metadata": {},
   "source": [
    "Funções secundárias utilizadas nas etapas principais da metodologia. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d176eb63",
   "metadata": {},
   "source": [
    "##### **Produto Interno**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "aaecc395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(a, b):\n",
    "    \"\"\"\n",
    "    Cálculo do produto interno dos casos:\n",
    "    - Vetor-vetor\n",
    "    - Matriz-vetor\n",
    "    - Matriz-matriz\n",
    "    \n",
    "    ### Parâmetros\n",
    "    - `a`/`b`: Vetor ou matriz.\n",
    "\n",
    "    ### Retorna\n",
    "    Float (escalar) para produto vetor-vetor / Lista (vetor) para produto matriz-vetor / Matriz (lista de listas) para produto matriz-matriz.\n",
    "    \"\"\"\n",
    "\n",
    "    # Funções auxiliares de identificação\n",
    "    def is_vector(x):\n",
    "        return isinstance(x[0], (int, float))\n",
    "\n",
    "    def is_matrix(x):\n",
    "        return isinstance(x[0], list)\n",
    "\n",
    "    # Produto vetor-vetor -> escalar\n",
    "    if is_vector(a) and is_vector(b):\n",
    "        if len(a) != len(b):\n",
    "            raise ValueError(\"Vetores devem ter o mesmo comprimento.\")\n",
    "        \n",
    "        s = 0\n",
    "        for i in range(len(a)):\n",
    "            s += a[i] * b[i]\n",
    "        return s\n",
    "\n",
    "    # Produto matriz-vetor -> vetor\n",
    "    if is_matrix(a) and is_vector(b):\n",
    "        n_rows = len(a)\n",
    "        n_cols = len(a[0])\n",
    "\n",
    "        if n_cols != len(b):\n",
    "            raise ValueError(\"Dimensões incompatíveis para produto matriz-vetor.\")\n",
    "\n",
    "        result = []\n",
    "        for i in range(n_rows):\n",
    "            sum = 0\n",
    "            for j in range(n_cols):\n",
    "                sum += a[i][j] * b[j]\n",
    "            result.append(sum)\n",
    "        return result\n",
    "\n",
    "    # Produto matriz-matriz -> matriz\n",
    "    if is_matrix(a) and is_matrix(b):\n",
    "        n_rows_a = len(a)\n",
    "        n_cols_a = len(a[0])\n",
    "        n_rows_b = len(b)\n",
    "        n_cols_b = len(b[0])\n",
    "\n",
    "        if n_cols_a != n_rows_b:\n",
    "            raise ValueError(\"Dimensões incompatíveis para produto matriz-matriz.\")\n",
    "\n",
    "        result = []\n",
    "        for i in range(n_rows_a):\n",
    "            row = []\n",
    "            for j in range(n_cols_b):\n",
    "                sum = 0\n",
    "                for k in range(n_cols_a):\n",
    "                    sum += a[i][k] * b[k][j]\n",
    "                row.append(sum)\n",
    "            result.append(row)\n",
    "        return result\n",
    "\n",
    "    # Caso inválido\n",
    "    raise TypeError(\"Entradas devem ser vetores ou matrizes válidas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d369a46a",
   "metadata": {},
   "source": [
    "##### ***Holdout***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "c0360d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout(X, y, train_size=0.8):\n",
    "    \"\"\"\n",
    "    Realiza a partição *holdout* de um conjunto de dados supervisionado\n",
    "    em subconjuntos de treinamento e teste.\n",
    "\n",
    "    A função separa aleatoriamente o conjunto de dados original\n",
    "    D = {(x_i, y_i)}_{i=1}^{N} em dois subconjuntos disjuntos:\n",
    "    um conjunto de treinamento D_train e um conjunto de teste D_test,\n",
    "    respeitando a proporção especificada por `train_size`.\n",
    "\n",
    "    ### Parâmetros\n",
    "    - `X` (list[list[float]]): Conjunto de N vetores de entrada, X = {x₁, x₂, …, x_N}, com x_i ∈ ℝᵈ.\n",
    "\n",
    "    - `y` (list[float]): Vetor dos valores alvo associados às instâncias de entrada, y = (y₁, y₂, …, y_N).\n",
    "\n",
    "    - `train_size` (float, opcional): Fração do conjunto de dados destinada ao treinamento, com 0 < `train_size` < 1.\n",
    "\n",
    "    ### Retorno\n",
    "    tuple[list, list, list, list]\n",
    "\n",
    "        Tupla `(X_train, X_test, y_train, y_test)`, onde:\n",
    "     - `X_train`: subconjunto dos vetores de entrada utilizados no treinamento;\n",
    "     - `X_test`: subconjunto dos vetores de entrada utilizados na avaliação;\n",
    "     - `y_train`: valores alvo correspondentes ao conjunto de treinamento;\n",
    "     - `y_test`: valores alvo correspondentes ao conjunto de teste.\n",
    "\n",
    "    ### Observações\n",
    "    - A partição é realizada por embaralhamento aleatório dos índices,\n",
    "      caracterizando um *holdout* simples.\n",
    "    - Não há estratificação em relação à variável alvo.\n",
    "    - Não há controle explícito de *seed*, o que implica não reprodutibilidade\n",
    "      entre execuções distintas.\n",
    "    - A função preserva a correspondência entre instâncias de entrada e seus\n",
    "      respectivos valores alvo.\n",
    "    \"\"\"\n",
    "\n",
    "    if not 0 < train_size < 1:\n",
    "        raise ValueError('train_size deve estar no intervalo (0, 1).')\n",
    "    \n",
    "    if len(X) != len(y):\n",
    "        raise ValueError('X e y devem ter o mesmo comprimento.')\n",
    "\n",
    "    n = len(X)\n",
    "    indices = list(range(n))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    split = int(n * train_size)\n",
    "\n",
    "    train_idx = indices[:split]\n",
    "    test_idx = indices[split:]\n",
    "\n",
    "    X_train = [X[i] for i in train_idx]\n",
    "    y_train = [y[i] for i in train_idx]\n",
    "\n",
    "    X_test = [X[i] for i in test_idx]\n",
    "    y_test = [y[i] for i in test_idx]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957cddad",
   "metadata": {},
   "source": [
    "### 0. Geração / Leitura de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeed561",
   "metadata": {},
   "source": [
    "O conjunto de dados é gerado a partir do modelo linear com ruído aditivo:\n",
    "$$\n",
    "y_i = \\mathbf{w}_{\\text{true}}^{\\top}\\mathbf{x}_i + b_{\\text{true}} + \\varepsilon_i,\n",
    "\\quad i = 1, \\dots, N\n",
    "$$\n",
    "\n",
    "Em que:\n",
    "- $\\mathbf{x}_i \\in \\mathbb{R}^d$: Vvetor de *features* da $i$-ésima amostra.\n",
    "- $\\mathbf{w}_{\\text{true}} \\in \\mathbb{R}^d$: Vetor de pesos verdadeiro.\n",
    "- $b_{\\text{true}} \\in \\mathbb{R}$: Viés verdadeiro.\n",
    "- $\\varepsilon_i \\sim \\mathcal{N}(0, 1)$: Ruído Gaussiano Aditivo Independente.\n",
    "\n",
    "Os vetores $\\mathbf{x}_i$ também são amostrados de uma Distribuição Normal Padrão:\n",
    "$$\n",
    "\\mathbf{x}_i \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "e78c3c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticLinearData:\n",
    "    def __init__(self, n_samples, n_features):\n",
    "        self.n_samples = n_samples\n",
    "        self.n_features = n_features\n",
    "    \n",
    "    def generate(self):\n",
    "        self.w_true = [random.gauss() for _ in range(self.n_features)]\n",
    "        self.b_true = random.uniform(0, 9)\n",
    "\n",
    "        X, y = [], []\n",
    "\n",
    "        for _ in range(self.n_samples):\n",
    "            x_i = [random.gauss() for _ in range(self.n_features)]\n",
    "            noise = random.gauss()\n",
    "            y_i = dot(self.w_true, x_i) + self.b_true + noise\n",
    "\n",
    "            X.append(x_i)\n",
    "            y.append(y_i)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff42c7",
   "metadata": {},
   "source": [
    "### 1. Definição do Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0c0b8",
   "metadata": {},
   "source": [
    "O modelo considerado corresponde a uma transformação afim totalmente conectada, definida por:\n",
    "$$\n",
    "\\hat{y} = \\mathbf{w}^{\\top}\\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "Em que:\n",
    "- $\\mathbf{x} \\in \\mathbb{R}^d$: Vetor de entrada.\n",
    "- $\\mathbf{w} \\in \\mathbb{R}^d$: Vetor de pesos treináveis.\n",
    "- $b \\in \\mathbb{R}$: Termo de viés.\n",
    "\n",
    "Os parâmetros $\\mathbf{w}$ e $b$ são inicializados aleatoriamente a partir de uma distribuição uniforme simétrica de pequeno suporte em torno de zero:\n",
    "$$\n",
    "w_j \\sim \\mathcal{U}(-\\varepsilon, \\varepsilon),\n",
    "\\quad\n",
    "b \\sim \\mathcal{U}(-\\varepsilon, \\varepsilon),\n",
    "\\quad\n",
    "\\varepsilon = 10^{-3}\n",
    "$$\n",
    "\n",
    "Essa abordagem tem por objetivo garantir estabilidade numérica e dinâmica adequada do gradiente nas etapas iniciais do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "cb2278f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressor:\n",
    "    def __init__(self, n_features):\n",
    "        self.n_features = n_features\n",
    "        self.w = [0.0] * n_features\n",
    "        self.b = 0.0\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return [dot(self.w, x) + self.b for x in X]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0ee940",
   "metadata": {},
   "source": [
    "### 2. *Forward Pass*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94b5a9f",
   "metadata": {},
   "source": [
    "Dado um conjunto de $N$ vetores de entrada, o *forward pass* do modelo linear consiste na aplicação de uma transformação afim independente a cada instância:\n",
    "$$\n",
    "\\hat{y}_i = \\mathbf{w}^{\\top}\\mathbf{x}_i + b, \\quad i = 1,\\,\\cdots,\\,N\n",
    "$$\n",
    "\n",
    "Em forma vetorial:\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = \\mathbf{Xw} + b\n",
    "$$\n",
    "\n",
    "Em que:\n",
    "- $X \\in \\mathbb{R}^{N \\times d}$: Matriz de entradas.\n",
    "- $\\mathbf{w} \\in \\mathbb{R}^{d}$: Vetor de pesos do modelo.\n",
    "- $b \\in \\mathbb{R}$: Termo de viés.\n",
    "- $\\hat{\\mathbf{y}} \\in \\mathbb{R}^{N}$: Vetor de saídas pré-ativação (*logits*).\n",
    "\n",
    "Cada saída $\\hat{y}_i$ corresponde à predição linear do modelo para a $i$-ésima amostra, sem aplicação de função de ativação não linear.\n",
    "\n",
    "**Observações conceituais**\n",
    "- Este *forward pass* implementa exatamente uma camada totalmente conectada linear, sendo matematicamente equivalente à regressão linear multivariada.\n",
    "- A ausência de função de ativação implica que o espaço de hipóteses do modelo é estritamente afim.\n",
    "- A independência entre as amostras reflete-se na forma escalar da operação aplicada a cada $\\mathbf{x}_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3420855",
   "metadata": {},
   "source": [
    "### 3. *Loss Function*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b43395",
   "metadata": {},
   "source": [
    "A verossimilhança das predições do modelo é avaliada por meio do Erro Quadrático Médio (*Mean Squared Error – MSE*), definido como:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{1}{N}  \\sum_{i=1}^{N} \\left(y_i - \\hat{y}_i\\right)^2\n",
    "$$\n",
    "\n",
    "Em que:\n",
    "- $\\mathbf{y} \\in \\mathbb{R}^{N}$: Vetor de valores verdadeiros (*targets*).\n",
    "- $\\hat{\\mathbf{y}} \\in \\mathbb{R}^{N}$: Vetor de predições do modelo.\n",
    "- $N$: Número total de amostras.\n",
    "\n",
    "O MSE penaliza erros grandes de forma quadrática, tornando-se particularmente sensível a desvios significativos entre predição e valor real.\n",
    "\n",
    "**Observações Conceituais**\n",
    "- A função de perda é convexa em relação a $\\hat{\\mathbf{y}}$ e, por composição, em relação aos parâmetros $\\mathbf{w}$ e $b$ de modelos lineares.\n",
    "- É diferenciável em todo o domínio, o que a torna apropriada para métodos de otimização baseados em gradiente.\n",
    "- Para Ruído Gaussiano Aditivo, a minimização do MSE equivale à estimação de máxima verossimilhança dos parâmetros do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "dd217e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError:\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        N = len(y_true)\n",
    "        return sum((y_true[i] - y_pred[i])**2 for i in range(N)) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c23989",
   "metadata": {},
   "source": [
    "### 4. *Backward Pass*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36a53a4",
   "metadata": {},
   "source": [
    "O *backward pass* do modelo linear consiste na avaliação explícita dos gradientes da função de perda em relação aos parâmetros treináveis. \n",
    "\n",
    "Considerando o modelo:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\mathbf{w}^{\\top}\\mathbf{x}_i + b\n",
    "$$\n",
    "\n",
    "E a função de perda Erro Quadrático Médio:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\left(y_i - \\hat{y}_i\\right)^2\n",
    "$$\n",
    "\n",
    "Os gradientes analíticos são dados por:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} \\mathcal{L} = \\frac{2}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)\\mathbf{x}_i, \\qquad \\frac{\\partial \\mathcal{L}}{\\partial b} = \n",
    "\\frac{2}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)\n",
    "$$\n",
    "\n",
    "Essas expressões resultam diretamente da aplicação da regra da cadeia e constituem a base para métodos de otimização por gradiente descendente.\n",
    "\n",
    "**Observações Conceituais**\n",
    "- Cada termo $(\\hat{y}_i - y_i)$ representa o erro de predição da $i$-ésima amostra.\n",
    "- O gradiente em relação aos pesos é uma soma ponderada dos vetores de entrada, escalada pelo erro.\n",
    "- O gradiente em relação ao viés corresponde à média dos erros, refletindo um deslocamento global da função afim.\n",
    "- A normalização por $N$ garante que a magnitude do gradiente seja independente do tamanho do conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "99464133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEGradient:\n",
    "    def backward(self, X, y_true, y_pred):\n",
    "\n",
    "        N = len(X)\n",
    "        d = len(X[0])\n",
    "\n",
    "        grad_w = [0.0] * d\n",
    "        grad_b = 0.0\n",
    "\n",
    "        for i in range(N):\n",
    "            error = y_true[i] - y_pred[i]\n",
    "\n",
    "            for j in range(d):\n",
    "                grad_w[j] += -2 * error * X[i][j]\n",
    "            \n",
    "            grad_b += -2 * error\n",
    "        \n",
    "        grad_w =[grad_w[k] / N for k in range(d)]\n",
    "        grad_b /= N\n",
    "\n",
    "        return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85df8d8",
   "metadata": {},
   "source": [
    "### 5. *Gradient Descent*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df27c4fb",
   "metadata": {},
   "source": [
    "A etapa de otimização do modelo é realizada por meio do Gradiente Descendente, que atualiza iterativamente os parâmetros treináveis na direção oposta ao gradiente da função de perda. Para um modelo linear, a regra de atualização é dada por:\n",
    "$$\n",
    "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta \\nabla_{\\mathbf{w}} \\mathcal{L}, \\qquad b^{(t+1)} = b^{(t)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b}\n",
    "$$\n",
    "\n",
    "Em que:\n",
    "- $\\eta > 0$ é a taxa de aprendizado (*learning rate*);\n",
    "- $\\nabla_{\\mathbf{w}} \\mathcal{L}$ e $\\frac{\\partial \\mathcal{L}}{\\partial b}$ são os gradientes computados no *backward pass*;\n",
    "- $t$ denota a iteração de treinamento.\n",
    "\n",
    "Essa atualização corresponde a um único passo de descida no espaço de parâmetros, com o objetivo de minimizar a função de perda.\n",
    "\n",
    "**Observações Conceituais**\n",
    "- O Gradiente Descendente explora o fato de que, em modelos lineares com MSE, a função de perda é convexa, assegurando convergência ao mínimo global sob escolha adequada de $\\eta$.\n",
    "- A magnitude da taxa de aprendizado controla o compromisso entre velocidade de convergência e estabilidade numérica.\n",
    "- A ausência de *momentum* ou mecanismos adaptativos torna essa implementação conceitualmente transparente e didaticamente valiosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "42b3114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, model, grad_w, grad_b):\n",
    "        model.w = [model.w[j] -self.lr * grad_w[j] for j in range(len(model.w))]\n",
    "\n",
    "        model.b -= self.lr * grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e96a94",
   "metadata": {},
   "source": [
    "### 6. *Training Loop*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea95ced3",
   "metadata": {},
   "source": [
    "O treinamento do modelo é realizado por meio de um *loop* iterativo de otimização baseada em gradiente, no qual os parâmetros do modelo são ajustados progressivamente para minimizar a função de perda Erro Quadrático Médio (MSE):\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w}, b) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(y_i - (\\mathbf{w}^{\\top}\\mathbf{x}_i + b)\\right)^2\n",
    "$$\n",
    "\n",
    "A cada época, o seguinte ciclo é executado:\n",
    "\n",
    "1. Inicialização dos parâmetros $(\\mathbf{w}, b)$ (realizada uma única vez).\n",
    "2. *Forward pass* para obtenção das predições $\\hat{\\mathbf{y}}$.\n",
    "3. Avaliação da perda $\\mathcal{L}$.\n",
    "4. *Backward pass* para cálculo analítico dos gradientes.\n",
    "5. Atualização dos parâmetros via Gradiente Descendente.\n",
    "\n",
    "De forma abstrata, o processo pode ser descrito como:\n",
    "\n",
    "$$\n",
    "(\\mathbf{w}, b) \\xleftarrow{\\text{iterativamente}} \\arg\\min_{\\mathbf{w}, b} \\mathcal{L}(\\mathbf{w}, b), \\quad \\text{via Gradiente Descendente em modo batch}.\n",
    "$$\n",
    "\n",
    "O vetor `loss_log` armazena o valor da função de perda ao final de cada época, permitindo a análise da dinâmica de convergência do treinamento.\n",
    "\n",
    "**Observações conceituais**\n",
    "- O treinamento é realizado em modo *batch*, utilizando todo o conjunto de dados em cada iteração.\n",
    "- Em modelos lineares com MSE, a função objetivo é convexa, garantindo convergência ao mínimo global sob escolha adequada da taxa de aprendizado.\n",
    "- A ausência de *early stopping*, regularização ou validação torna o procedimento conceitualmente simples e transparente, favorecendo análise didática do comportamento do gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "7037657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, loss_fn, loss_grad, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.loss_grad = loss_grad\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def fit(self, X, y, epochs):\n",
    "        loss_log = []\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            y_pred = self.model.forward(X)\n",
    "            loss = self.loss_fn(y, y_pred)\n",
    "            loss_log.append(loss)\n",
    "\n",
    "            grad_w, grad_b = self.loss_grad.backward(X, y, y_pred)\n",
    "            self.optimizer.step(self.model, grad_w, grad_b)\n",
    "\n",
    "        return loss_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63fb59",
   "metadata": {},
   "source": [
    "# Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "5830e65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desempenho (MSE) de teste do modelo: 0.9789565409999332\n"
     ]
    }
   ],
   "source": [
    "# Dados\n",
    "data = SyntheticLinearData(1_000, 2)\n",
    "X, y = data.generate()\n",
    "X_train, X_test, y_train, y_test = holdout(X, y)\n",
    "\n",
    "# Modelo\n",
    "model = LinearRegressor(n_features=2)\n",
    "\n",
    "# Componentes\n",
    "loss_fn = MeanSquaredError()\n",
    "loss_grad = MSEGradient()\n",
    "optimizer = GradientDescent(lr=0.001)\n",
    "\n",
    "# Treino\n",
    "trainer = Trainer(model, loss_fn, loss_grad, optimizer)\n",
    "loss_log = trainer.fit(X_train, y_train, epochs=10_000)\n",
    "\n",
    "# Avaliação\n",
    "train_loss = loss_log[-1]\n",
    "test_loss = loss_fn(y_test, model.predict(X_test))\n",
    "print(f'Desempenho (MSE) de teste do modelo: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0bcc2b",
   "metadata": {},
   "source": [
    "# Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e91ff",
   "metadata": {},
   "source": [
    "Comparação entre os parâmetros fundamentalmente verdadeiros definidos na geração dos dados sintéticos e os parâmetros estimados pelo modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "4c6047f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parâmetros verdadeiros:\n",
      "w_true = [-0.5183735347655064, 0.3892625131435465]\n",
      "b_true = 8.121953879779781\n",
      "\n",
      "Parâmetros aprendidos:\n",
      "w_hat  = [-0.5157789924964914, 0.42087750182010275]\n",
      "b_hat  = 8.075824447512753\n"
     ]
    }
   ],
   "source": [
    "print(\"Parâmetros verdadeiros:\")\n",
    "print(\"w_true =\", data.w_true)\n",
    "print(\"b_true =\", data.b_true)\n",
    "\n",
    "print(\"\\nParâmetros aprendidos:\")\n",
    "print(\"w_hat  =\", model.w)\n",
    "print(\"b_hat  =\", model.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fb7f3f",
   "metadata": {},
   "source": [
    "Visualização da evolução da *loss* do modelo ao longo das épocas de treino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "ba8d6d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(loss_log)\n",
    "\n",
    "ax.set_title('$Loss$ do Modelo ao Longo das Épocas')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_xlabel('Época');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cc4a53",
   "metadata": {},
   "source": [
    "# Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f84c35e",
   "metadata": {},
   "source": [
    "Conclusões gerais sobre o modelo são idênticas àquelas desenvolvidas para a versão 1.0 da implementação."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
